from argparse import ArgumentParser

from Model.ModelType import MODEL_LIST
from Dataset.DatasetType import DATASET_LIST
from Strategy.StrategyType import STRATEGY_LIST

from MultiLabelTrainer.DataReader import DataReader
from MultiLabelTrainer.MultiLabelDataset import MultiLabelDataset
from MultiLabelTrainer.Metric import compute_metrics
from MultiLabelTrainer.CustomTrainer import ConservativeTrainer

from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback

def parseArgs():
    parser = ArgumentParser()
    
    parser.add_argument("-m", "--model", nargs="+", choices=MODEL_LIST, help="choose your model")
    parser.add_argument("-d", "--dataset", nargs="+", choices=DATASET_LIST, help="choose your dataset")
    parser.add_argument("-s", "--strategy", nargs="+", choices=STRATEGY_LIST, help="choose your strategy")
    parser.add_argument("--dirpath", help="your dir path")
    parser.add_argument("--split", default=0.7, type=float, help="split train - val")
    parser.add_argument("--datanums", default=1000, type=int, help="total data nums")
    parser.add_argument("--maxlens", default=512, type=int, help="data max length")

    args = parser.parse_args()
    return args

def main():
    args = parseArgs()
    
    (train_texts, train_labels), (val_texts, val_labels) = DataReader(args.dirpath, args.model, args.dataset, args.strategy).getDataset(args.datanums, args.split)
    count = 0
    for label in train_labels:
        if label == [1, 1, 1, 1, 1] or label == [0, 0, 0, 0, 0]:
            count += 1
    print(f'{count} / {len(train_labels)}')
    model_name = "xlm-roberta-base"
    tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)
    train_dataset = MultiLabelDataset(train_texts, train_labels, tokenizer, args.maxlens)
    val_dataset = MultiLabelDataset(val_texts, val_labels, tokenizer, args.maxlens)
    
    model = XLMRobertaForSequenceClassification.from_pretrained(
        model_name, 
        num_labels=len(args.strategy), 
        problem_type="multi_label_classification" 
    )

    args = TrainingArguments(
        output_dir="xlm-roberta-multilabel-output3",
        eval_strategy="steps",    # ğŸ”¥ æ”¹æˆ steps
        eval_steps=100,
        save_strategy="steps",
        save_steps=100,
        learning_rate=2e-5,
        num_train_epochs=20,          # ğŸ”¥ ç›´æ¥è¨­å¤§ä¸€é» (ä¾‹å¦‚ 20)
        per_device_train_batch_size=64,
        per_device_eval_batch_size=64,
        fp16=True,
        weight_decay=0.05,
        load_best_model_at_end=True,
        metric_for_best_model="f0.5_micro", # å¤šæ¨™ç±¤é€šå¸¸çœ‹ F1-Micro

        logging_strategy="steps",
        logging_steps=50,  # æ¯ 10 æ­¥ç´€éŒ„ä¸€æ¬¡ Training Loss (ç•«åœ–æ¯”è¼ƒå¹³æ»‘)
        report_to="tensorboard"   # å…ˆè¨­ noneï¼Œæˆ‘å€‘ä¸‹é¢æ‰‹å‹•ç”¨ Matplotlib ç•«åœ–
    )

    trainer = ConservativeTrainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=15)]
    )

    # é–‹å§‹è¨“ç·´
    trainer.train()


if __name__ == '__main__':
    main()