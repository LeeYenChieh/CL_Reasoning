from sklearn.metrics import accuracy_score, fbeta_score, accuracy_score, precision_score
import torch
import numpy as np

import numpy as np
from sklearn.metrics import fbeta_score, accuracy_score, precision_score
import torch

def multi_label_metrics(predictions, labels, threshold=0.5):
    # 1. è½‰æˆ Sigmoid æ©Ÿç‡
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))
    
    # 2. è½‰æˆ 0/1 é æ¸¬
    y_pred = np.zeros(probs.shape)
    y_pred[probs >= threshold] = 1
    
    # --- ğŸ”¥ é—œéµä¿®æ­£é–‹å§‹ ---
    # Scikit-learn ä¸æ”¯æ´ -100ï¼Œæˆ‘å€‘å¿…é ˆæ‰‹å‹•éæ¿¾æ‰
    
    # å»ºç«‹é®ç½©ï¼šæ‰¾å‡ºå“ªäº›æ¨™ç±¤ä¸æ˜¯ -100 (æœ‰æ•ˆæ¨™ç±¤)
    mask = (labels != -100)
    
    # åˆ©ç”¨é®ç½©å–å‡ºæœ‰æ•ˆè³‡æ–™ (é€™æœƒè‡ªå‹•æŠŠäºŒç¶­é™£åˆ—æ”¤å¹³æˆä¸€ç¶­)
    # ä¾‹å¦‚: labels=[[1, -100], [0, 1]] -> filtered_labels=[1, 0, 1]
    y_true_filtered = labels[mask]
    y_pred_filtered = y_pred[mask]
    
    # --- ğŸ”¥ é—œéµä¿®æ­£çµæŸ ---

    # 3. è¨ˆç®—æŒ‡æ¨™ (ä½¿ç”¨éæ¿¾å¾Œçš„è³‡æ–™)
    # å› ç‚ºå·²ç¶“æ”¤å¹³æˆä¸€ç¶­äº†ï¼Œé€™è£¡ç®—å‡ºä¾†çš„å…¶å¯¦å°±æ˜¯ Micro çš„æ¦‚å¿µ (Global count)
    
    if len(y_true_filtered) == 0:
        return {
            "f0.5_micro": 0.0,
            "precision": 0.0,
            "accuracy": 0.0
        }

    f05_micro = fbeta_score(y_true_filtered, y_pred_filtered, beta=0.5, average='micro')
    precision = precision_score(y_true_filtered, y_pred_filtered, average='micro', zero_division=0)
    
    # æ³¨æ„ï¼šé€™è£¡çš„ Accuracy è®Šæˆäº†ã€Œæ ¼å­ç´šåˆ¥ã€çš„æº–ç¢ºç‡ (Cell-wise Accuracy)
    # ä¹Ÿå°±æ˜¯ï¼š(æ‰€æœ‰çŒœå°çš„æ ¼å­æ•¸) / (æ‰€æœ‰æœ‰æ•ˆæ ¼å­æ•¸)
    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)
    
    return {
        "f0.5_micro": f05_micro,
        "precision": precision,
        "accuracy": accuracy
    }

def compute_metrics(p):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    result = multi_label_metrics(predictions=preds, labels=p.label_ids)
    return result