import torch
from torch import nn
from transformers import Trainer

class ConservativeTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        # --- 1. å»ºç«‹é®ç½© (Mask) ---
        # å‡è¨­ -100 ä»£è¡¨ç¼ºå¤±è³‡æ–™ (Missing Label)
        # mask ç‚º 1 ä»£è¡¨æœ‰æ•ˆï¼Œç‚º 0 ä»£è¡¨ç„¡æ•ˆ
        mask = (labels != -100).float()
        
        # --- 2. æ¸…æ´—æ¨™ç±¤ (Clean Labels) ---
        # BCEWithLogitsLoss ç„¡æ³•è¨ˆç®— -100ï¼Œæœƒå°è‡´ NaN
        # æˆ‘å€‘æŠŠ -100 æš«æ™‚è®Šç‚º 0 (åæ­£ç­‰ä¸€ä¸‹æœƒè¢« mask ä¹˜ä»¥ 0 æŠµéŠ·æ‰ï¼Œæ”¹æˆ 1 ä¹Ÿå¯ä»¥)
        # .clamp(min=0) æœƒæŠŠæ‰€æœ‰è² æ•¸è®Šæˆ 0
        clean_labels = labels.clamp(min=0)
        
        # --- 3. è¨­å®šæ¬Šé‡ (Pos Weight) ---
        # é€™è£¡è¨­å®š 0.6 (æ¯” 1 å°ï¼Œç¨å¾®æ‡²ç½°èª¤å ±ï¼Œé¼“å‹µä¿å®ˆ)
        weights = torch.full((logits.shape[1],), 0.6).to(logits.device) 
        
        # --- 4. è¨ˆç®—åŸå§‹ Loss (ä¸å¹³å‡) ---
        # ğŸ”¥ é—œéµï¼šè¨­å®š reduction='none'ï¼Œé€™æ¨£å®ƒæœƒå›å‚³ä¸€å€‹è·Ÿ logits å½¢ç‹€ä¸€æ¨£çš„ loss çŸ©é™£
        # è€Œä¸æ˜¯ç›´æ¥å›å‚³ä¸€å€‹å¹³å‡æ•¸
        loss_fct = nn.BCEWithLogitsLoss(pos_weight=weights, reduction='none')
        loss = loss_fct(logits, clean_labels)
        
        # --- 5. å¥—ç”¨é®ç½©ä¸¦è¨ˆç®—å¹³å‡ ---
        # æŠŠç„¡æ•ˆè³‡æ–™çš„ Loss è®Šæˆ 0
        masked_loss = loss * mask
        
        # ç®—å‡ºå¹³å‡ Loss
        # åˆ†æ¯æ˜¯ã€Œæœ‰æ•ˆè³‡æ–™çš„ç¸½æ•¸ (mask.sum())ã€ï¼ŒåŠ ä¸Š 1e-9 é˜²æ­¢é™¤ä»¥é›¶
        final_loss = masked_loss.sum() / (mask.sum() + 1e-9)
        
        return (final_loss, outputs) if return_outputs else final_loss